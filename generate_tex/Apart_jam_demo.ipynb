{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaron-sandoval/ai-gen-papers/blob/main/Apart_jam_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT2prQLfWuiP",
        "outputId": "426b1033-f15d-4299-854c-9f46bac229c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install scipy --quiet\n",
        "!pip install tenacity --quiet\n",
        "!pip install tiktoken==0.3.3 --quiet\n",
        "!pip install termcolor --quiet\n",
        "!pip install openai --quiet\n",
        "!pip install arxiv --quiet\n",
        "!pip install pandas --quiet\n",
        "!pip install PyPDF2 --quiet\n",
        "!pip install tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass('Enter the OpenAI API Key: ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsGIq-gYZL34",
        "outputId": "c729758e-8557-4f95-e0fc-29a0261e9456"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import arxiv\n",
        "import ast\n",
        "import concurrent\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from csv import writer\n",
        "from IPython.display import display, Markdown, Latex\n",
        "from openai import OpenAI\n",
        "from PyPDF2 import PdfReader\n",
        "from scipy import spatial\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "\n",
        "GPT_MODEL = \"gpt-4o-mini-2024-07-18\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "VQHwPsXAXEO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = './data/papers'\n",
        "\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(directory):\n",
        "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
        "    os.makedirs(directory)\n",
        "    print(f\"Directory '{directory}' created successfully.\")\n",
        "else:\n",
        "    # If the directory already exists, print a message indicating it\n",
        "    print(f\"Directory '{directory}' already exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61r6LKZUZbHB",
        "outputId": "61cc0ccc-9e51-4cd8-97be-2b428339a7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory './data/papers' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a directory to store downloaded papers\n",
        "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
        "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
        "\n",
        "# Generate a blank dataframe where we can store downloaded files\n",
        "df = pd.DataFrame(list())\n",
        "df.to_csv(paper_dir_filepath)"
      ],
      "metadata": {
        "id": "RsBS47jDZkN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def embedding_request(text):\n",
        "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
        "    return response\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
        "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
        "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
        "    \"\"\"\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query = query,\n",
        "        max_results = 10,\n",
        "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
        "    )\n",
        "    result_list = []\n",
        "    for result in client.results(search):\n",
        "        result_dict = {}\n",
        "        result_dict.update({\"title\": result.title})\n",
        "        result_dict.update({\"summary\": result.summary})\n",
        "\n",
        "        # Taking the first url provided\n",
        "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
        "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
        "        result_list.append(result_dict)\n",
        "\n",
        "        # Store references in library file\n",
        "        response = embedding_request(text=result.title)\n",
        "        file_reference = [\n",
        "            result.title,\n",
        "            result.download_pdf(data_dir),\n",
        "            response.data[0].embedding,\n",
        "        ]\n",
        "\n",
        "        # Write to file\n",
        "        with open(library, \"a\") as f_object:\n",
        "            writer_object = writer(f_object)\n",
        "            writer_object.writerow(file_reference)\n",
        "            f_object.close()\n",
        "    return result_list"
      ],
      "metadata": {
        "id": "961v3u5YZqdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test that the search is working\n",
        "result_output = get_articles(\"LLM Safety\")\n",
        "result_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzZctdeoZwMX",
        "outputId": "975197db-879a-46e3-88b3-9e22207e5ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Controllable Text Generation for Large Language Models: A Survey',\n",
              " 'summary': \"In Natural Language Processing (NLP), Large Language Models (LLMs) have\\ndemonstrated high text generation quality. However, in real-world applications,\\nLLMs must meet increasingly complex requirements. Beyond avoiding misleading or\\ninappropriate content, LLMs are also expected to cater to specific user needs,\\nsuch as imitating particular writing styles or generating text with poetic\\nrichness. These varied demands have driven the development of Controllable Text\\nGeneration (CTG) techniques, which ensure that outputs adhere to predefined\\ncontrol conditions--such as safety, sentiment, thematic consistency, and\\nlinguistic style--while maintaining high standards of helpfulness, fluency, and\\ndiversity.\\n  This paper systematically reviews the latest advancements in CTG for LLMs,\\noffering a comprehensive definition of its core concepts and clarifying the\\nrequirements for control conditions and text quality. We categorize CTG tasks\\ninto two primary types: content control and attribute control. The key methods\\nare discussed, including model retraining, fine-tuning, reinforcement learning,\\nprompt engineering, latent space manipulation, and decoding-time intervention.\\nWe analyze each method's characteristics, advantages, and limitations,\\nproviding nuanced insights for achieving generation control. Additionally, we\\nreview CTG evaluation methods, summarize its applications across domains, and\\naddress key challenges in current research, including reduced fluency and\\npracticality. We also propose several appeals, such as placing greater emphasis\\non real-world applications in future research. This paper aims to offer\\nvaluable guidance to researchers and developers in the field. Our reference\\nlist and Chinese version are open-sourced at\\nhttps://github.com/IAAR-Shanghai/CTGSurvey.\",\n",
              " 'article_url': 'http://arxiv.org/abs/2408.12599v1',\n",
              " 'pdf_url': 'http://arxiv.org/pdf/2408.12599v1'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data/papers/*"
      ],
      "metadata": {
        "id": "ok_SNrx-bDL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100,\n",
        ") -> list[str]:\n",
        "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
        "    query_embedding_response = embedding_request(query)\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n]"
      ],
      "metadata": {
        "id": "afvnJtFmalk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(filepath):\n",
        "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
        "    # creating a pdf reader object\n",
        "    reader = PdfReader(filepath)\n",
        "    pdf_text = \"\"\n",
        "    page_number = 0\n",
        "    for page in reader.pages:\n",
        "        page_number += 1\n",
        "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
        "    return pdf_text\n",
        "\n",
        "\n",
        "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
        "def create_chunks(text, n, tokenizer):\n",
        "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
        "        j = min(i + int(1.5 * n), len(tokens))\n",
        "        while j > i + int(0.5 * n):\n",
        "            # Decode the tokens and check for full stop or newline\n",
        "            chunk = tokenizer.decode(tokens[i:j])\n",
        "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
        "                break\n",
        "            j -= 1\n",
        "        # If no end of sentence found, use n tokens as the chunk size\n",
        "        if j == i + int(0.5 * n):\n",
        "            j = min(i + n, len(tokens))\n",
        "        yield tokens[i:j]\n",
        "        i = j\n",
        "\n",
        "def extract_chunk(content, template_prompt):\n",
        "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
        "    prompt = template_prompt + content\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def summarize_text(query):\n",
        "    \"\"\"This function does the following:\n",
        "    - Reads in the arxiv_library.csv file in including the embeddings\n",
        "    - Finds the closest file to the user's query\n",
        "    - Scrapes the text out of the file and chunks it\n",
        "    - Summarizes each chunk in parallel\n",
        "    - Does one final summary and returns this to the user\"\"\"\n",
        "\n",
        "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
        "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
        "\n",
        "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
        "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    if len(library_df) == 0:\n",
        "        print(\"No papers searched yet, downloading first.\")\n",
        "        get_articles(query)\n",
        "        print(\"Papers downloaded, continuing\")\n",
        "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
        "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
        "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
        "    print(\"Chunking text from paper\")\n",
        "    pdf_text = read_pdf(strings[0])\n",
        "\n",
        "    # Initialise tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    results = \"\"\n",
        "\n",
        "    # Chunk up the document into 1500 token chunks\n",
        "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
        "    print(\"Summarizing each chunk of text\")\n",
        "\n",
        "    # Parallel process the summaries\n",
        "    with concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=len(text_chunks)\n",
        "    ) as executor:\n",
        "        futures = [\n",
        "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
        "            for chunk in text_chunks\n",
        "        ]\n",
        "        with tqdm(total=len(text_chunks)) as pbar:\n",
        "            for _ in concurrent.futures.as_completed(futures):\n",
        "                pbar.update(1)\n",
        "        for future in futures:\n",
        "            data = future.result()\n",
        "            results += data\n",
        "\n",
        "    # Final summary\n",
        "    print(\"Summarizing into overall summary\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
        "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
        "                        User query: {query}\n",
        "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
        "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "D5TzOFoCbjxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the summarize_text function works\n",
        "chat_test_response = summarize_text(\"LLM Safety\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piKPhFPNbzaA",
        "outputId": "051038bb-a8e1-4b32-f84d-522fe41ff07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:11<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing into overall summary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_test_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLAbQDHlcABj",
        "outputId": "ec8ccca3-a7bc-44ba-9368-63166eb7f65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Summary of LLM Safety in Medical Education\n",
            "\n",
            "**Core Argument:**\n",
            "- The paper argues for the integration of Large Language Models (LLMs) into medical education through a multi-agent framework called MEDCO (Medical Education Copilots). This approach addresses the limitations of existing AI tools, which often rely on solitary learning methods and fail to replicate the interactive, multidisciplinary nature of real-world medical training.\n",
            "\n",
            "**Evidence:**\n",
            "- **Limitations of Current Tools:** Existing AI educational tools primarily use single chatbots, restricting the learning experience to one role at a time, which does not reflect the complexity of medical training.\n",
            "- **Multi-Agent Framework:** MEDCO incorporates three agents: an agentic patient, an expert doctor, and a radiologist, facilitating a more realistic and collaborative learning environment.\n",
            "- **Performance Improvement:** Experiments show that students trained with MEDCO achieve significant performance improvements, comparable to advanced models, and enhance peer discussions, further improving diagnostic performance.\n",
            "- **Hierarchical Evaluation Metrics:** The authors propose new metrics for assessing diagnostic accuracy, allowing for a nuanced understanding of student performance.\n",
            "- **Case-Based Learning:** The framework allows students to engage in interactive scenarios, fostering inquiry and collaboration, which are essential for developing clinical reasoning skills.\n",
            "\n",
            "**Conclusions:**\n",
            "- MEDCO represents a significant advancement in medical education by providing an interactive and collaborative training approach, essential for developing competent medical professionals.\n",
            "- The integration of LLMs in medical education through frameworks like MEDCO can enhance learning experiences, improve diagnostic skills, and prepare students for real-world medical challenges.\n",
            "- Future research should explore the effectiveness of MEDCO in aiding human students and its potential applications in other educational contexts beyond medicine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3-N7mtqhC2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}